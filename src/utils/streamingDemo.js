// Streaming Demo for eDEX Chatbot
// This file demonstrates how to use the streaming functionality

/**
 * Demo function to show streaming capabilities
 * This would typically be called from the renderer process
 */
class StreamingDemo {
  /**
   * Run a demo conversation showing streaming
   */
  static async runDemo() {
    console.log("ðŸš€ Starting streaming demo...");
    
    // Example of how to use the streaming functionality
    const demoMessages = [
      "Hello! Can you explain what token-by-token streaming is?",
      "That's interesting. How does it improve user experience?",
      "What are the technical challenges in implementing streaming?",
      "Thanks for the detailed explanation!"
    ];
    
    // Simulate sending messages with delays
    for (let i = 0; i < demoMessages.length; i++) {
      console.log(`\nðŸ“¤ Sending message ${i + 1}: "${demoMessages[i]}"`);
      
      // In a real implementation, you would call:
      // await window.chat.sendMessage(demoMessages[i]);
      
      // Simulate streaming response
      await this.simulateStreamingResponse(demoMessages[i], i);
      
      // Add delay between messages
      if (i < demoMessages.length - 1) {
        await this.delay(2000);
      }
    }
    
    console.log("\nâœ… Demo completed!");
  }
  
  /**
   * Simulate a streaming response
   * @param {string} userMessage - The user's message
   * @param {number} index - Message index
   */
  static async simulateStreamingResponse(userMessage, index) {
    // Example responses for demo
    const responses = [
      "Token-by-token streaming is a technique where the AI response is sent to the client as individual tokens (words or parts of words) are generated, rather than waiting for the complete response. This creates a more interactive and responsive user experience, similar to how ChatGPT works. Each token is immediately displayed as it's generated by the language model.",
      "Streaming significantly improves user experience in several ways: 1) It reduces perceived latency by showing content immediately, 2) It creates a more natural conversation flow, 3) Users can start reading the response before it's complete, 4) It provides better feedback that the system is working, 5) It enables real-time interaction with long responses.",
      "Implementing streaming has several technical challenges: 1) Managing the connection state and handling interruptions, 2) Properly buffering and flushing tokens for optimal performance, 3) Handling different streaming protocols (SSE, chunked responses), 4) Implementing graceful error handling and retries, 5) Ensuring security and preventing abuse, 6) Managing client-side rendering performance with frequent updates, 7) Supporting cancellation of ongoing streams.",
      "You're welcome! I hope this explanation was helpful. Token-by-token streaming is a powerful technique that makes AI interactions feel more natural and responsive. If you have any more questions about streaming or AI implementation, feel free to ask!"
    ];
    
    const response = responses[index] || "This is a simulated streaming response.";
    
    console.log("ðŸ“¥ Receiving streaming response:");
    console.log("--- START OF STREAM ---");
    
    // Simulate streaming by sending tokens one by one
    const tokens = response.split(/(?<=\s)/); // Split by whitespace but keep the spaces
    let accumulatedResponse = "";
    
    for (let i = 0; i < tokens.length; i++) {
      const token = tokens[i];
      accumulatedResponse += token;
      
      // Simulate network delay
      await this.delay(20);
      
      // Display the token as it would appear in streaming
      process.stdout.write(token);
    }
    
    console.log("\n--- END OF STREAM ---");
  }
  
  /**
   * Delay function for simulation
   * @param {number} ms - Milliseconds to delay
   */
  static delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
  
  /**
   * Example of how to integrate with the Chat class
   * This is how you would use it in the actual application
   */
  static exampleIntegration() {
    console.log("ðŸ“˜ Example Integration:");
    console.log(`
// 1. Initialize the chat component (already done in _renderer.js)
const chat = new Chat({
  parentId: "chat_feed",
  onmessage: handleChatMessage
});

// 2. Send a message with streaming (called from handleSendMessage)
chat.sendMessage("Explain quantum computing in simple terms", "openai", "gpt-4o-mini", {}, true);

// 3. Handle streaming events in the message handler
function handleChatMessage(event) {
  switch(event.type) {
    case 'stream_start':
      console.log("Stream started");
      showCancelButton();
      break;
    case 'stream_content':
      updateStreamingMessage(event.data); // Update UI with new token
      break;
    case 'stream_end':
      console.log("Stream completed");
      hideCancelButton();
      showToast("Response received");
      break;
    case 'error':
      console.error("Streaming error:", event.data);
      hideCancelButton();
      showToast("Error: " + event.data, "error");
      break;
  }
}

// 4. Cancel streaming if needed
function handleCancelStreaming() {
  chat.cancelStreaming();
  hideCancelButton();
  showSendButton();
}
    `);
  }
  
  /**
   * Show the message schema
   */
  static showMessageSchema() {
    console.log("ðŸ“‹ Message Schema:");
    console.log(`
interface ChatMessage {
  id: string;               // unique id (uuid)
  role: "user" | "assistant" | "system" | "tool";
  content: string;          // progressively filled for assistant
  timestamp: number;
  isStreaming?: boolean;    // true while tokens are arriving
}
    `);
  }
}

// Export for both Node.js and browser environments
if (typeof module !== 'undefined' && module.exports) {
  module.exports = { StreamingDemo };
} else {
  window.StreamingDemo = StreamingDemo;
}